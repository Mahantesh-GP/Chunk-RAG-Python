{
  "summary": {
    "fixed": {
      "avg_response_time": 0.006662050882975261,
      "avg_relevancy": 0.027562609940599215,
      "avg_faithfulness": 0.5833333333333334,
      "count": 3
    },
    "structure": {
      "avg_response_time": 0.005333105723063151,
      "avg_relevancy": 0.03455125845569693,
      "avg_faithfulness": 0.3611111111111111,
      "count": 3
    },
    "semantic": {
      "avg_response_time": 0.005191326141357422,
      "avg_relevancy": 0.059274396089473845,
      "avg_faithfulness": 0.16111111111111112,
      "count": 3
    },
    "multigranular": {
      "avg_response_time": 0.0053323109944661455,
      "avg_relevancy": 0.027562609940599215,
      "avg_faithfulness": 0.5833333333333334,
      "count": 3
    }
  },
  "results": [
    {
      "query": "What is Leaking bucket algorithm?",
      "strategy": "fixed",
      "response_time": 0.006113290786743164,
      "relevancy": 0.02032253236548679,
      "faithfulness": 0.5,
      "retrieved_chunks": [
        "helps to choose the right algorithm or combination of algorithms to fit our use cases. Here is a list of popular algorithms: \u2022 Token bucket \u2022 Leaking bucket \u2022 Fixed window counter \u2022 Sliding window log \u2022 Sliding window counter Token bucket algorithm The token bucket algorithm is widely used for rate limiting. It is simple, well understood and commonly used by internet companies. Both Amazon [5] and Stripe [6] use this algorithm to throttle their API requests. The token bucket algorithm work as follows: \u2022 A token bucket is a container that has pre-defined capacity. Tokens are put in the bucket at preset rates periodically. Once the bucket is full, no more tokens are added. As shown in Figure 4-4, the token bucket capacity is 4. The refiller puts 2 tokens into the bucket every second. Once the bucket is full, extra tokens will overflow. \u2022 Each request consumes one token. When a request arrives, we check if there are enough tokens in the bucket. Figure 4-5 explains how it works. \u2022 If there are enough tokens, we take one token out for each request, and the request goes through. \u2022 If there are not enough tokens, the request is dropped. Figure 4-6 illustrates how token consumption, refill , and rate limiting logic work. In this example, the token bucket size is 4, and the refill rate is 4 per 1 minute. The token bucket algorithm takes two parameters: \u2022 Bucket size: the maximum number of tokens allowed in the bucket \u2022 Refill rate: number",
        "queue is full. If it is not full, the request is added to the queue. \u2022 Otherwise, the request is dropped. \u2022 Requests are pulled from the queue and processed at regular intervals. Figure 4-7 explains how the algorithm works. Leaking bucket algorithm takes the following two parameters: \u2022 Bucket size: it is equal to the queue size. The queue holds the requests to be processed at a fixed rate. \u2022 Outflow rate: it defines how many requests can be processed at a fixed rate, usually in seconds. Shopify, an ecommerce company, uses leaky buckets for rate-limiting [7]. Pros: \u2022 Memory efficient given the limited queue size. \u2022 Requests are processed at a fixed rate therefore it is suitable for use cases that a stable outflow rate is needed. Cons: \u2022 A burst of traffic fills up the queue with old requests, and if they are not processed in time, recent requests will be rate limited. \u2022 There are two parameters in the algorithm. It might not be easy to tune them properly. Fixed window counter algorithm Fixed window counter algorithm works as follows: \u2022 The algorithm divides the timeline into fix-sized time windows and assign a counter for each window. \u2022 Each request increments the counter by one. \u2022 Once the counter reaches the pre-defined threshold, new requests are dropped until a new time window starts. Let us use a concrete example to see how it works. In Figure 4-8, the time unit is 1 second and the system allows a maximum of 3 requests",
        "algorithm takes two parameters: \u2022 Bucket size: the maximum number of tokens allowed in the bucket \u2022 Refill rate: number of tokens put into the bucket every second How many buckets do we need? This varies, and it depends on the rate-limiting rules. Here are a few examples. \u2022 It is usually necessary to have different buckets for different API endpoints. For instance, if a user is allowed to make 1 post per second, add 150 friends per day, and like 5 posts per second, 3 buckets are required for each user. \u2022 If we need to throttle requests based on IP addresses, each IP address requires a bucket. \u2022 If the system allows a maximum of 10,000 requests per second, it makes sense to have a global bucket shared by all requests. Pros: \u2022 The algorithm is easy to implement. \u2022 Memory efficient. \u2022 Token bucket allows a burst of traffic for short periods. A request can go through as long as there are tokens left. Cons: \u2022 Two parameters in the algorithm are bucket size and token refill rate. However, it might be challenging to tune them properly. Leaking bucket algorithm The leaking bucket algorithm is similar to the token bucket except that requests are processed at a fixed rate. It is usually implemented with a first-in-first-out (FIFO) queue. The algorithm works as follows: \u2022 When a request arrives, the system checks if the queue is full. If it is not full, the request is added to the queue. \u2022 Otherwise, the request is",
        "proportional to the differences between the two replicas , and not the amount of data they contain. In real-world systems, the bucket size is quite big. For instance, a possible configuration is one million buckets per one billion keys, so each bucket only contains 1000 keys. Handling data center outage Data center outage could happen due to power outage, network outage, natural disaster, etc. To build a system capable of handling data center outage, it is important to replicate data across multiple data centers. Even if a data center is completely offline, users can still access data through the other data centers. System architecture diagram Now that we have discussed different technical considerations in designing a key-value store, we can shift our focus on the architecture diagram, shown in Figure 6-17. Main features of the architecture are listed as follows: \u2022 Clients communicate with the key-value store through simple APIs: get(key) and put(key, value) . \u2022 A coordinator is a node that acts as a proxy between the client and the key-value store. \u2022 Nodes are distributed on a ring using consistent hashing. \u2022 The system is completely decentralized so adding and moving nodes can be automatic. \u2022 Data is replicated at multiple nodes. \u2022 There is no single point of failure as every node has the same set of responsibilities. As the design is decentralized, each node performs many tasks as presented in Figure 6-18. Write path Figure 6-19 explains what happens after a write request is directed to a specific node. Please note the",
        "web presents an unprecedented opportunity for data mining. Web mining helps to discover useful knowledge from the internet. For example, top financial firms use crawlers to download shareholder meetings and annual reports to learn key company initiatives. \u2022 Web monitoring. The crawlers help to monitor copyright and trademark infringements over the Internet. For example, Digimarc [3] utilizes crawlers to discover pirated works and reports. The complexity of developing a web crawler depends on the scale we intend to support. It could be either a small school project, which takes only a few hours to complete or a gigantic project that requires continuous improvement from a dedicated engineering team. Thus, we will explore the scale and features to support below . Step 1 - Understand the problem and establish design scope The basic algorithm of a web crawler is simple: 1. Given a set of URLs, download all the web pages addressed by the URLs. 2. Extract URLs from these web pages 3. Add new URLs to the list of URLs to be downloaded. Repeat these 3 steps. Does a web crawler work truly as simple as this basic algorithm? Not exactly. Designing a vastly scalable web crawler is an extremely complex task. It is unlikely for anyone to design a massive web crawler within the interview duration. Before jumping into the design, we must ask questions to understand the requirements and establish design scope: Candidate : What is the main purpose of the crawler? Is it used for search engine indexing, data mining, or something else?"
      ]
    },
    {
      "query": "How to DESIGN A RATE LIMITER?",
      "strategy": "fixed",
      "response_time": 0.007472991943359375,
      "relevancy": 0.03518259532109676,
      "faithfulness": 0.8,
      "retrieved_chunks": [
        "Propose high-level design and get buy-in Let us keep things simple and use a basic client and server model for communication. Where to put the rate limiter? Intuitively, you can implement a rate limiter at either the client or server-side. \u2022 Client-side implementation. Generally speaking, client is an unreliable place to enforce rate limiting because client requests can easily be forged by malicious actors. Moreover, we might not have control over the client implementation. \u2022 Server-side implementation. Figure 4-1 shows a rate limiter that is placed on the server- side. Besides the client and server-side implementations, there is an alternative way. Instead of putting a rate limiter at the API servers, we create a rate limiter middleware, which throttles requests to your APIs as shown in Figure 4-2. Let us use an example in Figure 4-3 to illustrate how rate limiting works in this design. Assume our API allows 2 requests per second, and a client sends 3 requests to the server within a second. The first two requests are routed to API servers. However, the rate limiter middleware throttles the third request and returns a HTTP status code 429. The HTTP 429 response status code indicates a user has sent too many requests. Cloud microservices [4] have become widely popular and rate limiting is usually implemented within a component called API gateway. API gateway is a fully managed service that supports rate limiting, SSL termination, authentication, IP whitelisting, servicing static content, etc. For now, we only need to know that the API gateway is a",
        "feed is slim. Most users are only interested in the latest content, so the cache miss rate is low. 5. Store <post_id, user_id > in news feed cache. Figure 11-6 shows an example of what the news feed looks like in cache. Newsfeed retrieval deep dive Figure 11-7 illustrates the detailed design for news feed retrieval. As shown in Figure 11-7, media content (images, videos, etc.) are stored in CDN for fast retrieval. Let us look at how a client retrieves news feed. 1. A user sends a request to retrieve her news feed. The request looks like this: /v1/me/feed 2. The load balancer redistributes requests to web servers. 3. Web servers call the news feed service to fetch news feeds. 4. News feed service gets a list post IDs from the news feed cache. 5. A user\u2019s news feed is more than just a list of feed IDs. It contains username, profile picture, post content, post image, etc. Thus, the news feed service fetches the complete user and post objects from caches (user cache and post cache) to construct the fully hydrated news feed. 6. The fully hydrated news feed is returned in JSON format back to the client for rendering. Cache architecture Cache is extremely important for a news feed system. We divide the cache tier into 5 layers as shown in Figure 11-8. \u2022 News Feed: It stores IDs of news feeds. \u2022 Content: It stores every post data. Popular content is stored in hot cache. \u2022 Social Graph: It stores user relationship",
        "limiting. It is an in- memory store that offers two commands: INCR and EXPIRE. \u2022 INCR: It increases the stored counter by 1. \u2022 EXPIRE: It sets a timeout for the counter. If the timeout expires, the counter is automatically deleted. Figure 4-12 shows the high-level architecture for rate limiting, and this works as follows: \u2022 The client sends a request to rate limiting middleware. \u2022 Rate limiting middleware fetches the counter from the corresponding bucket in Redis and checks if the limit is reached or not. \u2022 If the limit is reached, the request is rejected. \u2022 If the limit is not reached, the request is sent to API servers. Meanwhile, the system increments the counter and saves it back to Redis. Step 3 - Design deep dive The high-level design in Figure 4-12 does not answer the following questions: \u2022 How are rate limiting rules created? Where are the rules stored? \u2022 How to handle requests that are rate limited? In this section, we will first answer the questions regarding rate limiting rules and then go over the strategies to handle rate-limited requests. Finally, we will discuss rate limiting in distributed environment, a detailed design, performance optimization and monitoring. Rate limiting rules Lyft open-sourced their rate-limiting component [12]. We will peek inside of the component and look at some examples of rate limiting rules: domain : messaging descriptors : - key : message_type Value : marketing rate_limit : unit : day requests_per_unit : 5 In the above example, the system is configured to allow",
        ": marketing rate_limit : unit : day requests_per_unit : 5 In the above example, the system is configured to allow a maximum of 5 marketing messages per day. Here is another example: domain : auth descriptors : - key : auth_type Value : login rate_limit : unit : minute requests_per_unit : 5 This rule shows that clients are not allowed to login more than 5 times in 1 minute. Rules are generally written in configuration files and saved on disk. Exceeding the rate limit In case a request is rate limited, APIs return a HTTP response code 429 (too many requests) to the client. Depending on the use cases, we may enqueue the rate-limited requests to be processed later. For example, if some orders are rate limited due to system overload, we may keep those orders to be processed later. Rate limiter headers How does a client know whether it is being throttled? And how does a client know the number of allowed remaining requests before being throttled? The answer lies in HTTP response headers. The rate limiter returns the following HTTP headers to clients: X-Ratelimit-Remaining : The remaining number of allowed requests within the window. X-Ratelimit-Limit: It indicates how many calls the client can make per time window. X-Ratelimit-Retry-After: The number of seconds to wait until you can make a request again without being throttled. When a user has sent too many requests, a 429 too many requests error and X-Ratelimit- Retry-After header are returned to the client. Detailed design Figure 4-13 presents a detailed",
        "server-side API rate limiter? Interviewer : Great question. We focus on the server-side API rate limiter. Candidate : Does the rate limiter throttle API requests based on IP, the user ID, or other properties? Interviewer : The rate limiter should be flexible enough to support different sets of throttle rules. Candidate : What is the scale of the system? Is it built for a startup or a big company with a large user base? Interviewer : The system must be able to handle a large number of requests. Candidate : Will the system work in a distributed environment? Interviewer : Yes. Candidate : Is the rate limiter a separate service or should it be implemented in application code? Interviewer : It is a design decision up to you. Candidate : Do we need to inform users who are throttled? Interviewer : Yes. Requirements Here is a summary of the requirements for the system: \u2022 Accurately limit excessive requests. \u2022 Low latency. The rate limiter should not slow down HTTP response time. \u2022 Use as little memory as possible. \u2022 Distributed rate limiting. The rate limiter can be shared across multiple servers or processes. \u2022 Exception handling. Show clear exceptions to users when their requests are throttled. \u2022 High fault tolerance. If there are any problems with the rate limiter (for example, a cache server goes offline), it does not affect the entire system. Step 2 - Propose high-level design and get buy-in Let us keep things simple and use a basic client and server model for"
      ]
    },
    {
      "query": "What is Sliding window log algorithm?",
      "strategy": "fixed",
      "response_time": 0.006399869918823242,
      "relevancy": 0.0271827021352141,
      "faithfulness": 0.45,
      "retrieved_chunks": [
        "it works. In Figure 4-8, the time unit is 1 second and the system allows a maximum of 3 requests per second. In each second window, if more than 3 requests are received, extra requests are dropped as shown in Figure 4-8. A major problem with this algorithm is that a burst of traffic at the edges of time windows could cause more requests than allowed quota to go through. Consider the following case: In Figure 4-9, the system allows a maximum of 5 requests per minute, and the available quota resets at the human-friendly round minute. As seen, there are five requests between 2:00:00 and 2:01:00 and five more requests between 2:01:00 and 2:02:00. For the one-minute window between 2:00:30 and 2:01:30, 10 requests go through. That is twice as many as allowed requests. Pros: \u2022 Memory efficient. \u2022 Easy to understand. \u2022 Resetting available quota at the end of a unit time window fits certain use cases. Cons: \u2022 Spike in traffic at the edges of a window could cause more requests than the allowed quota to go through. Sliding window log algorithm As discussed previously, the fixed window counter algorithm has a major issue: it allows more requests to go through at the edges of a window. The sliding window log algorithm fixes the issue. It works as follows: \u2022 The algorithm keeps track of request timestamps. Timestamp data is usually kept in cache, such as sorted sets of Redis [8]. \u2022 When a new request comes in, remove all the outdated timestamps.",
        "removed from the log. After the remove operation, the log size becomes 2; therefore, the request is accepted. Pros: \u2022 Rate limiting implemented by this algorithm is very accurate. In any rolling window, requests will not exceed the rate limit. Cons: \u2022 The algorithm consumes a lot of memory because even if a request is rejected, its timestamp might still be stored in memory. Sliding window counter algorithm The sliding window counter algorithm is a hybrid approach that combines the fixed window counter and sliding window log. The algorithm can be implemented by two different approaches. We will explain one implementation in this section and provide reference for the other implementation at the end of the section. Figure 4-11 illustrates how this algorithm works. Assume the rate limiter allows a maximum of 7 requests per minute, and there are 5 requests in the previous minute and 3 in the current minute. For a new request that arrives at a 30% position in the current minute, the number of requests in the rolling window is calculated using the following formula: \u2022 Requests in current window + requests in the previous window * overlap percentage of the rolling window and previous window \u2022 Using this formula, we get 3 + 5 * 0.7% = 6.5 request. Depending on the use case, the number can either be rounded up or down. In our example, it is rounded down to 6. Since the rate limiter allows a maximum of 7 requests per minute, the current request can go through. However,",
        "helps to choose the right algorithm or combination of algorithms to fit our use cases. Here is a list of popular algorithms: \u2022 Token bucket \u2022 Leaking bucket \u2022 Fixed window counter \u2022 Sliding window log \u2022 Sliding window counter Token bucket algorithm The token bucket algorithm is widely used for rate limiting. It is simple, well understood and commonly used by internet companies. Both Amazon [5] and Stripe [6] use this algorithm to throttle their API requests. The token bucket algorithm work as follows: \u2022 A token bucket is a container that has pre-defined capacity. Tokens are put in the bucket at preset rates periodically. Once the bucket is full, no more tokens are added. As shown in Figure 4-4, the token bucket capacity is 4. The refiller puts 2 tokens into the bucket every second. Once the bucket is full, extra tokens will overflow. \u2022 Each request consumes one token. When a request arrives, we check if there are enough tokens in the bucket. Figure 4-5 explains how it works. \u2022 If there are enough tokens, we take one token out for each request, and the request goes through. \u2022 If there are not enough tokens, the request is dropped. Figure 4-6 illustrates how token consumption, refill , and rate limiting logic work. In this example, the token bucket size is 4, and the refill rate is 4 per 1 minute. The token bucket algorithm takes two parameters: \u2022 Bucket size: the maximum number of tokens allowed in the bucket \u2022 Refill rate: number",
        "traffic like flash sales. In this scenario, we may replace the algorithm to support burst traffic. Token bucket is a good fit here. Step 4 - Wrap up In this chapter, we discussed different algorithms of rate limiting and their pros/cons. Algorithms discussed include: \u2022 Token bucket \u2022 Leaking bucket \u2022 Fixed window \u2022 Sliding window log \u2022 Sliding window counter Then, we discussed the system architecture, rate limiter in a distributed environment, performance optimization and monitoring. Similar to any system design interview questions, there are additional talking points you can mention if time allows: \u2022 Hard vs soft rate limiting. \u2022 Hard: The number of requests cannot exceed the threshold. \u2022 Soft: Requests can exceed the threshold for a short period. \u2022 Rate limiting at different levels. In this chapter, we only talked about rate limiting at the application level (HTTP: layer 7). It is possible to apply rate limiting at other layers. For example, you can apply rate limiting by IP addresses using Iptables [15] (IP: layer 3). Note: The Open Systems Interconnection model (OSI model) has 7 layers [16]: Layer 1: Physical layer, Layer 2: Data link layer, Layer 3: Network layer, Layer 4: Transport layer, Layer 5: Session layer, Layer 6: Presentation layer, Layer 7: Application layer. \u2022 Avoid being rate limited. Design your client with best practices: \u2022 Use client cache to avoid making frequent API calls. \u2022 Understand the limit and do not send too many requests in a short time frame. \u2022 Include code to catch exceptions or errors",
        "Write path Figure 6-19 explains what happens after a write request is directed to a specific node. Please note the proposed designs for write/read paths are primary based on the architecture of Cassandra [8]. 1. The write request is persisted on a commit log file. 2. Data is saved in the memory cache. 3. When the memory cache is full or reaches a predefined threshold, data is flushed to SSTable [9] on disk. Note: A sorted-string table (SSTable) is a sorted list of <key, value> pairs. For readers interested in learning more about SStable, refer to the reference material [9]. Read path After a read request is directed to a specific node, it first checks if data is in the memory cache. If so, the data is returned to the client as shown in Figure 6-20. If the data is not in memory, it will be retrieved from the disk instead. We need an efficient way to find out which SSTable contains the key. Bloom filter [10] is commonly used to solve this problem. The read path is shown in Figure 6-21 when data is not in memory. 1. The system first checks if data is in memory. If not, go to step 2. 2. If data is not in memory, the system checks the bloom filter. 3. The bloom filter is used to figure out which SSTables might contain the key. 4. SSTables return the result of the data set. 5. The result of the data set is returned to the client. Summary This chapter"
      ]
    },
    {
      "query": "What is Leaking bucket algorithm?",
      "strategy": "structure",
      "response_time": 0.005000114440917969,
      "relevancy": 0.026312392214831237,
      "faithfulness": 0.25,
      "retrieved_chunks": [
        "as a new master and bring up a new slave node. \u2022 Slave down: If a slave is down, you can use another slave for read operations and",
        "explore 1 on 1 chat flow, message synchronization across multiple devices and group chat\nflow.\n1 on 1 chat flow\nFigure 12-12 explains what happens when User A sends a message to User B.\n1. User A sends a chat message to Chat server 1.\n2. Chat server 1 obtains a message ID from the ID generator.\n3. Chat server 1 sends the message to the message sync queue.\n4. The message is stored in a key-value store.\n5.a. If User B is online, the message is forwarded to Chat server 2 where User B is\nconnected.\n5.b. If User B is offline, a push notification is sent from push notification (PN) servers.\n6. Chat server 2 forwards the message to User B. There is a persistent WebSocket\nconnection between User B and Chat server 2.\nMessage synchronization across multiple devices\nMany users have multiple devices. We will explain how to sync messages across multiple\ndevices. Figure 12-13 shows an example of message synchronization.",
        "Hinted handoff is used to handle temporary failures. What if a replica is permanently\nunavailable? To handle such a situation, we implement an anti-entropy protocol to keep\nreplicas in sync. Anti-entropy involves comparing each piece of data on replicas and updating\neach replica to the newest version. A Merkle tree is used for inconsistency detection and\nminimizing the amount of data transferred.\nQuoted from Wikipedia [7]: \u201cA hash tree or Merkle tree is a tree in\n \nwhich every non-leaf\nnode is labeled with the hash of the labels or values (in case of leaves) of its child nodes.\nHash trees allow efficient and secure verification of the contents of large data structures\u201d.\nAssuming key space is from 1 to 12, the following steps show how to build a Merkle tree.\nHighlighted boxes indicate inconsistency.\nStep 1: Divide key space into buckets (4 in our example) as shown in Figure 6-13.  A bucket\nis used as the root level node to maintain a limited depth of the tree.\nStep 2: Once the buckets are created, hash each key in a bucket using a uniform hashing\nmethod (Figure 6-14).\nStep 3: Create a single hash node per bucket (Figure 6-15).\nStep 4: Build the tree upwards till root by calculating hashes of children (Figure 6-16).",
        "commonly used by internet companies. Both Amazon [5] and Stripe [6] use this algorithm to\nthrottle their API requests.\nThe token bucket algorithm work as follows:\n\u2022\nA token bucket is a container that has pre-defined capacity. Tokens are put in the bucket\nat preset rates periodically. Once the bucket is full, no more tokens are added. As shown in\nFigure 4-4, the token bucket capacity is 4. The refiller puts 2 tokens into the bucket every\nsecond. Once the bucket is full, extra tokens will overflow.\n\u2022\nEach request consumes one token. When a request arrives, we check if there are enough\ntokens in the bucket. Figure 4-5 explains how it works.\n\u2022\nIf there are enough tokens, we take one token out for each request, and the request\ngoes through.\n\u2022\nIf there are not enough tokens, the request is dropped.",
        "Second, synchronize data with an eventual consistency model. If you are unclear about the\neventual consistency model, refer to the \u201cConsistency\u201d section in \u201cChapter 6: Design a Key-\nvalue Store.\u201d\nMonitoring\nAfter the rate limiter is put in place, it is important to gather analytics data to check whether\nthe rate limiter is effective. Primarily, we want to make sure:\n\u2022\nThe rate limiting algorithm is effective.\n\u2022\nThe rate limiting rules are effective.\nFor example, if rate limiting rules are too strict, many valid requests are dropped. In this case,\nwe want to relax the rules a little bit. In another example, we notice our rate limiter becomes\nineffective when there is a sudden increase in traffic like flash sales. In this scenario, we may\nreplace the algorithm to support burst traffic. Token bucket is a good fit here."
      ]
    },
    {
      "query": "How to DESIGN A RATE LIMITER?",
      "strategy": "structure",
      "response_time": 0.005998849868774414,
      "relevancy": 0.04786395220678637,
      "faithfulness": 0.5333333333333333,
      "retrieved_chunks": [
        "Retry-After \nheader are returned to the client.\nDetailed design\nFigure 4-13 presents a detailed design of the system.\n\u2022\nRules are stored on the disk. Workers frequently pull rules from the disk and store them\nin the cache.\n\u2022\nWhen a client sends a request to the server, the request is sent to the rate limiter\nmiddleware first.\n\u2022\nRate limiter middleware loads rules from the cache. It fetches counters and last request\ntimestamp from Redis cache. Based on the response, the rate limiter decides:\n\u2022\nif the request is not rate limited, it is forwarded to API servers.\n\u2022\nif the request is rate limited, the rate limiter returns 429 too many requests error to\nthe client. In the meantime, the request is either dropped or forwarded to the queue.",
        "globe (Figure 14-24). People in the United States can upload videos to the North America\nupload center, and people in China can upload videos to the Asian upload center. To achieve\nthis, we use CDN as upload centers.\nSpeed optimization: parallelism everywhere\nAchieving low latency requires serious efforts. Another optimization is to build a loosely\ncoupled system and enable high parallelism.\nOur design needs some modifications to achieve high parallelism. Let us zoom in to the flow\nof how a video is transferred from original storage to the CDN. The flow is shown in Figure\n14-25, revealing that the output depends on the input of the previous step. This dependency\nmakes parallelism difficult.\nTo make the system more loosely coupled, we introduced message queues as shown in Figure",
        "Step 2 - Propose high-level design and get buy-in\nLet us keep things simple and use a basic client and server model for communication.\nWhere to put the rate limiter?\nIntuitively, you can implement a rate limiter at either the client or server-side. \n\u2022\nClient-side implementation. Generally speaking, client is an unreliable place to enforce\nrate limiting because client requests can easily be forged by malicious actors. Moreover,\nwe might not have control over the client implementation.\n\u2022\nServer-side implementation. Figure 4-1 shows a rate limiter that is placed on the server-\nside.\nBesides the client and server-side implementations, there is an alternative way. Instead of\nputting a rate limiter at the API servers, we create a rate limiter middleware, which throttles\nrequests to your APIs as shown in Figure 4-2.\nLet us use an example in Figure 4-3 to illustrate how rate limiting works in this design.\nAssume our API allows 2 requests per second, and a client sends 3 requests to the server\nwithin a second. The first two requests are routed to API servers. However, the rate limiter\nmiddleware throttles the third request and returns a HTTP status code 429. The HTTP 429\nresponse status code indicates a user has sent too many requests.",
        "store and cache are not in a single transaction. When scaling across multiple regions, maintaining consistency between",
        "Second, synchronize data with an eventual consistency model. If you are unclear about the\neventual consistency model, refer to the \u201cConsistency\u201d section in \u201cChapter 6: Design a Key-\nvalue Store.\u201d\nMonitoring\nAfter the rate limiter is put in place, it is important to gather analytics data to check whether\nthe rate limiter is effective. Primarily, we want to make sure:\n\u2022\nThe rate limiting algorithm is effective.\n\u2022\nThe rate limiting rules are effective.\nFor example, if rate limiting rules are too strict, many valid requests are dropped. In this case,\nwe want to relax the rules a little bit. In another example, we notice our rate limiter becomes\nineffective when there is a sudden increase in traffic like flash sales. In this scenario, we may\nreplace the algorithm to support burst traffic. Token bucket is a good fit here."
      ]
    },
    {
      "query": "What is Sliding window log algorithm?",
      "strategy": "structure",
      "response_time": 0.00500035285949707,
      "relevancy": 0.029477430945473206,
      "faithfulness": 0.3,
      "retrieved_chunks": [
        "as a new master and bring up a new slave node. \u2022 Slave down: If a slave is down, you can use another slave for read operations and",
        "\u2022\nEasy to understand.\n\u2022\nResetting available quota at the end of a unit time window fits certain use cases.\nCons:\n\u2022\nSpike in traffic at the edges of a window could cause more requests than the allowed\nquota to go through.\nSliding window log algorithm\nAs discussed previously, the fixed window counter algorithm has a major issue: it allows\nmore requests to go through at the edges of a window. The sliding window log algorithm\nfixes the issue. It works as follows:\n\u2022\nThe algorithm keeps track of request timestamps. Timestamp data is usually kept in\ncache, such as sorted sets of Redis [8].\n\u2022\nWhen a new request comes in, remove all the outdated timestamps. Outdated timestamps\nare defined as those older than the start of the current time window.\n\u2022\nAdd timestamp of the new request to the log.\n\u2022\nIf the log size is the same or lower than the allowed count, a request is accepted.\nOtherwise, it is rejected.\nWe explain the algorithm with an example as revealed in Figure 4-10.\nIn this example, the rate limiter allows 2 requests per minute. Usually, Linux timestamps are\nstored in the log. However, human-readable representation of time is used in our example for\nbetter readability.\n\u2022\nThe log is empty when a new request arrives at 1:00:01. Thus, the request is allowed.",
        "\u2022\nA new request arrives at 1:00:30, the timestamp 1:00:30 is inserted into the log. After the\ninsertion, the log size is 2, not larger than the allowed count. Thus, the request is allowed.\n\u2022\nA new request arrives at 1:00:50, and the timestamp is inserted into the log. After the\ninsertion, the log size is 3, larger than the allowed size 2. Therefore, this request is rejected\neven though the timestamp remains in the log.\n\u2022\nA new request arrives at 1:01:40. Requests in the range [1:00:40,1:01:40) are within the\nlatest time frame, but requests sent before 1:00:40 are outdated. Two outdated timestamps,\n1:00:01 and 1:00:30, are removed from the log. After the remove operation, the log size\nbecomes 2; therefore, the request is accepted.\nPros:\n\u2022\nRate limiting implemented by this algorithm is very accurate. In any rolling window,\nrequests will not exceed the rate limit.\nCons:\n\u2022\nThe algorithm consumes a lot of memory because even if a request is rejected, its\ntimestamp might still be stored in memory.\nSliding window counter algorithm\nThe sliding window counter algorithm is a hybrid approach that combines the fixed window\ncounter and sliding window log. The algorithm can be implemented by two different\napproaches. We will explain one implementation in this section and provide reference for the\nother implementation at the end of the section. Figure 4-11 illustrates how this algorithm\nworks.\nAssume the rate limiter allows a maximum of 7 requests per minute, and there are 5 requests\nin the previous minute and 3 in the current minute. For a new request that arrives at a 30%\nposition in the current minute, the number of requests in the rolling window is calculated\nusing the following formula:\n\u2022\nRequests in current window \n+ \nrequests in the previous window\n * \noverlap percentage of\nthe rolling window and previous window",
        "Write path\nFigure 6-19 explains what happens after a write request is directed to a specific node. Please\nnote the proposed designs for write/read paths are primary based on the architecture of\nCassandra [8].\n1. The write request is persisted on a commit log file. \n2. Data is saved in the memory cache.\n\n3. When the memory cache is full or reaches a predefined threshold, data is flushed to\nSSTable [9] on disk. Note: A sorted-string table (SSTable) is a sorted list of <key, value>\npairs. For readers interested in learning more about SStable, refer to the reference material\n[9].\nRead path\nAfter a read request is directed to a specific node, it first checks if data is in the memory\ncache. If so, the data is returned to the client as shown in Figure 6-20.\nIf the data is not in memory, it will be retrieved from the disk instead. We need an efficient\nway to find out which SSTable contains the key. Bloom filter [10] is commonly used to solve\nthis problem.\nThe read path is shown in Figure 6-21 when data is not in memory.\n1. The system first checks if data is in memory. If not, go to step 2.\n2. If data is not in memory, the system checks the bloom filter.",
        "explore 1 on 1 chat flow, message synchronization across multiple devices and group chat\nflow.\n1 on 1 chat flow\nFigure 12-12 explains what happens when User A sends a message to User B.\n1. User A sends a chat message to Chat server 1.\n2. Chat server 1 obtains a message ID from the ID generator.\n3. Chat server 1 sends the message to the message sync queue.\n4. The message is stored in a key-value store.\n5.a. If User B is online, the message is forwarded to Chat server 2 where User B is\nconnected.\n5.b. If User B is offline, a push notification is sent from push notification (PN) servers.\n6. Chat server 2 forwards the message to User B. There is a persistent WebSocket\nconnection between User B and Chat server 2.\nMessage synchronization across multiple devices\nMany users have multiple devices. We will explain how to sync messages across multiple\ndevices. Figure 12-13 shows an example of message synchronization."
      ]
    },
    {
      "query": "What is Leaking bucket algorithm?",
      "strategy": "semantic",
      "response_time": 0.004999876022338867,
      "relevancy": 0.05289072039072039,
      "faithfulness": 0.1,
      "retrieved_chunks": [
        "Figure 4-6 illustrates how token consumption, refill\n,\n and rate limiting logic work. In this\nexample, the token bucket size is 4, and the refill rate is 4 per 1 minute.",
        "Long polling\nBecause polling could be inefficient, the next progression is long polling (Figure 12-4).",
        "As the data grows every day, your database gets more overloaded. It is time to scale the data\ntier.",
        "Write path\nFigure 6-19 explains what happens after a write request is directed to a specific node. Please\nnote the proposed designs for write/read paths are primary based on the architecture of\nCassandra [8].\n1. The write request is persisted on a commit log file. \n2. Data is saved in the memory cache.",
        "so it does not support failover and redundancy. Database replication is a common technique\nto address those problems. Let us take a look."
      ]
    },
    {
      "query": "How to DESIGN A RATE LIMITER?",
      "strategy": "semantic",
      "response_time": 0.004999876022338867,
      "relevancy": 0.07643956043956043,
      "faithfulness": 0.3333333333333333,
      "retrieved_chunks": [
        "In Figure 13-10, each trie node on the left is mapped to the \n<key, value\n>\n pair on the right. If\nyou are unclear how key-value stores work, refer to Chapter 6: Design a key-value store.\nQuery service\nIn the high-level design, query service calls the database directly to fetch the top 5 results.\nFigure 13-11 shows the improved design as previous design is inefficient.",
        "so it does not support failover and redundancy. Database replication is a common technique\nto address those problems. Let us take a look.",
        "CHAPTER 5: DESIGN CONSISTENT HASHING\nTo achieve horizontal scaling, it is important to distribute requests/data efficiently and evenly\nacross servers. Consistent hashing is a commonly used technique to achieve this goal. But\nfirst, let us take an in-depth look at the problem.",
        "Newsfeed retrieval deep dive\nFigure 11-7 illustrates the detailed design for news feed retrieval.\nAs shown in Figure 11-7, media content (images, videos, etc.) are stored in CDN for fast\nretrieval. Let us look at how a client retrieves news feed.",
        "CHAPTER 8: DESIGN A URL SHORTENER\nIn this chapter, we will tackle an interesting and classic system design interview question:\ndesigning a URL shortening service like tinyurl."
      ]
    },
    {
      "query": "What is Sliding window log algorithm?",
      "strategy": "semantic",
      "response_time": 0.005574226379394531,
      "relevancy": 0.048492907438140706,
      "faithfulness": 0.05,
      "retrieved_chunks": [
        "Write path\nFigure 6-19 explains what happens after a write request is directed to a specific node. Please\nnote the proposed designs for write/read paths are primary based on the architecture of\nCassandra [8].\n1. The write request is persisted on a commit log file. \n2. Data is saved in the memory cache.",
        "Long polling\nBecause polling could be inefficient, the next progression is long polling (Figure 12-4).",
        "As the data grows every day, your database gets more overloaded. It is time to scale the data\ntier.",
        "so it does not support failover and redundancy. Database replication is a common technique\nto address those problems. Let us take a look.",
        "3. The bloom filter is used to figure out which SSTables might contain the key.\n4. SSTables return the result of the data set.\n5. The result of the data set is returned to the client."
      ]
    },
    {
      "query": "What is Leaking bucket algorithm?",
      "strategy": "multigranular",
      "response_time": 0.006000041961669922,
      "relevancy": 0.02032253236548679,
      "faithfulness": 0.5,
      "retrieved_chunks": [
        "helps to choose the right algorithm or combination of algorithms to fit our use cases. Here is a list of popular algorithms: \u2022 Token bucket \u2022 Leaking bucket \u2022 Fixed window counter \u2022 Sliding window log \u2022 Sliding window counter Token bucket algorithm The token bucket algorithm is widely used for rate limiting. It is simple, well understood and commonly used by internet companies. Both Amazon [5] and Stripe [6] use this algorithm to throttle their API requests. The token bucket algorithm work as follows: \u2022 A token bucket is a container that has pre-defined capacity. Tokens are put in the bucket at preset rates periodically. Once the bucket is full, no more tokens are added. As shown in Figure 4-4, the token bucket capacity is 4. The refiller puts 2 tokens into the bucket every second. Once the bucket is full, extra tokens will overflow. \u2022 Each request consumes one token. When a request arrives, we check if there are enough tokens in the bucket. Figure 4-5 explains how it works. \u2022 If there are enough tokens, we take one token out for each request, and the request goes through. \u2022 If there are not enough tokens, the request is dropped. Figure 4-6 illustrates how token consumption, refill , and rate limiting logic work. In this example, the token bucket size is 4, and the refill rate is 4 per 1 minute. The token bucket algorithm takes two parameters: \u2022 Bucket size: the maximum number of tokens allowed in the bucket \u2022 Refill rate: number",
        "queue is full. If it is not full, the request is added to the queue. \u2022 Otherwise, the request is dropped. \u2022 Requests are pulled from the queue and processed at regular intervals. Figure 4-7 explains how the algorithm works. Leaking bucket algorithm takes the following two parameters: \u2022 Bucket size: it is equal to the queue size. The queue holds the requests to be processed at a fixed rate. \u2022 Outflow rate: it defines how many requests can be processed at a fixed rate, usually in seconds. Shopify, an ecommerce company, uses leaky buckets for rate-limiting [7]. Pros: \u2022 Memory efficient given the limited queue size. \u2022 Requests are processed at a fixed rate therefore it is suitable for use cases that a stable outflow rate is needed. Cons: \u2022 A burst of traffic fills up the queue with old requests, and if they are not processed in time, recent requests will be rate limited. \u2022 There are two parameters in the algorithm. It might not be easy to tune them properly. Fixed window counter algorithm Fixed window counter algorithm works as follows: \u2022 The algorithm divides the timeline into fix-sized time windows and assign a counter for each window. \u2022 Each request increments the counter by one. \u2022 Once the counter reaches the pre-defined threshold, new requests are dropped until a new time window starts. Let us use a concrete example to see how it works. In Figure 4-8, the time unit is 1 second and the system allows a maximum of 3 requests",
        "algorithm takes two parameters: \u2022 Bucket size: the maximum number of tokens allowed in the bucket \u2022 Refill rate: number of tokens put into the bucket every second How many buckets do we need? This varies, and it depends on the rate-limiting rules. Here are a few examples. \u2022 It is usually necessary to have different buckets for different API endpoints. For instance, if a user is allowed to make 1 post per second, add 150 friends per day, and like 5 posts per second, 3 buckets are required for each user. \u2022 If we need to throttle requests based on IP addresses, each IP address requires a bucket. \u2022 If the system allows a maximum of 10,000 requests per second, it makes sense to have a global bucket shared by all requests. Pros: \u2022 The algorithm is easy to implement. \u2022 Memory efficient. \u2022 Token bucket allows a burst of traffic for short periods. A request can go through as long as there are tokens left. Cons: \u2022 Two parameters in the algorithm are bucket size and token refill rate. However, it might be challenging to tune them properly. Leaking bucket algorithm The leaking bucket algorithm is similar to the token bucket except that requests are processed at a fixed rate. It is usually implemented with a first-in-first-out (FIFO) queue. The algorithm works as follows: \u2022 When a request arrives, the system checks if the queue is full. If it is not full, the request is added to the queue. \u2022 Otherwise, the request is",
        "proportional to the differences between the two replicas , and not the amount of data they contain. In real-world systems, the bucket size is quite big. For instance, a possible configuration is one million buckets per one billion keys, so each bucket only contains 1000 keys. Handling data center outage Data center outage could happen due to power outage, network outage, natural disaster, etc. To build a system capable of handling data center outage, it is important to replicate data across multiple data centers. Even if a data center is completely offline, users can still access data through the other data centers. System architecture diagram Now that we have discussed different technical considerations in designing a key-value store, we can shift our focus on the architecture diagram, shown in Figure 6-17. Main features of the architecture are listed as follows: \u2022 Clients communicate with the key-value store through simple APIs: get(key) and put(key, value) . \u2022 A coordinator is a node that acts as a proxy between the client and the key-value store. \u2022 Nodes are distributed on a ring using consistent hashing. \u2022 The system is completely decentralized so adding and moving nodes can be automatic. \u2022 Data is replicated at multiple nodes. \u2022 There is no single point of failure as every node has the same set of responsibilities. As the design is decentralized, each node performs many tasks as presented in Figure 6-18. Write path Figure 6-19 explains what happens after a write request is directed to a specific node. Please note the",
        "web presents an unprecedented opportunity for data mining. Web mining helps to discover useful knowledge from the internet. For example, top financial firms use crawlers to download shareholder meetings and annual reports to learn key company initiatives. \u2022 Web monitoring. The crawlers help to monitor copyright and trademark infringements over the Internet. For example, Digimarc [3] utilizes crawlers to discover pirated works and reports. The complexity of developing a web crawler depends on the scale we intend to support. It could be either a small school project, which takes only a few hours to complete or a gigantic project that requires continuous improvement from a dedicated engineering team. Thus, we will explore the scale and features to support below . Step 1 - Understand the problem and establish design scope The basic algorithm of a web crawler is simple: 1. Given a set of URLs, download all the web pages addressed by the URLs. 2. Extract URLs from these web pages 3. Add new URLs to the list of URLs to be downloaded. Repeat these 3 steps. Does a web crawler work truly as simple as this basic algorithm? Not exactly. Designing a vastly scalable web crawler is an extremely complex task. It is unlikely for anyone to design a massive web crawler within the interview duration. Before jumping into the design, we must ask questions to understand the requirements and establish design scope: Candidate : What is the main purpose of the crawler? Is it used for search engine indexing, data mining, or something else?"
      ]
    },
    {
      "query": "How to DESIGN A RATE LIMITER?",
      "strategy": "multigranular",
      "response_time": 0.004998207092285156,
      "relevancy": 0.03518259532109676,
      "faithfulness": 0.8,
      "retrieved_chunks": [
        "Propose high-level design and get buy-in Let us keep things simple and use a basic client and server model for communication. Where to put the rate limiter? Intuitively, you can implement a rate limiter at either the client or server-side. \u2022 Client-side implementation. Generally speaking, client is an unreliable place to enforce rate limiting because client requests can easily be forged by malicious actors. Moreover, we might not have control over the client implementation. \u2022 Server-side implementation. Figure 4-1 shows a rate limiter that is placed on the server- side. Besides the client and server-side implementations, there is an alternative way. Instead of putting a rate limiter at the API servers, we create a rate limiter middleware, which throttles requests to your APIs as shown in Figure 4-2. Let us use an example in Figure 4-3 to illustrate how rate limiting works in this design. Assume our API allows 2 requests per second, and a client sends 3 requests to the server within a second. The first two requests are routed to API servers. However, the rate limiter middleware throttles the third request and returns a HTTP status code 429. The HTTP 429 response status code indicates a user has sent too many requests. Cloud microservices [4] have become widely popular and rate limiting is usually implemented within a component called API gateway. API gateway is a fully managed service that supports rate limiting, SSL termination, authentication, IP whitelisting, servicing static content, etc. For now, we only need to know that the API gateway is a",
        "feed is slim. Most users are only interested in the latest content, so the cache miss rate is low. 5. Store <post_id, user_id > in news feed cache. Figure 11-6 shows an example of what the news feed looks like in cache. Newsfeed retrieval deep dive Figure 11-7 illustrates the detailed design for news feed retrieval. As shown in Figure 11-7, media content (images, videos, etc.) are stored in CDN for fast retrieval. Let us look at how a client retrieves news feed. 1. A user sends a request to retrieve her news feed. The request looks like this: /v1/me/feed 2. The load balancer redistributes requests to web servers. 3. Web servers call the news feed service to fetch news feeds. 4. News feed service gets a list post IDs from the news feed cache. 5. A user\u2019s news feed is more than just a list of feed IDs. It contains username, profile picture, post content, post image, etc. Thus, the news feed service fetches the complete user and post objects from caches (user cache and post cache) to construct the fully hydrated news feed. 6. The fully hydrated news feed is returned in JSON format back to the client for rendering. Cache architecture Cache is extremely important for a news feed system. We divide the cache tier into 5 layers as shown in Figure 11-8. \u2022 News Feed: It stores IDs of news feeds. \u2022 Content: It stores every post data. Popular content is stored in hot cache. \u2022 Social Graph: It stores user relationship",
        "limiting. It is an in- memory store that offers two commands: INCR and EXPIRE. \u2022 INCR: It increases the stored counter by 1. \u2022 EXPIRE: It sets a timeout for the counter. If the timeout expires, the counter is automatically deleted. Figure 4-12 shows the high-level architecture for rate limiting, and this works as follows: \u2022 The client sends a request to rate limiting middleware. \u2022 Rate limiting middleware fetches the counter from the corresponding bucket in Redis and checks if the limit is reached or not. \u2022 If the limit is reached, the request is rejected. \u2022 If the limit is not reached, the request is sent to API servers. Meanwhile, the system increments the counter and saves it back to Redis. Step 3 - Design deep dive The high-level design in Figure 4-12 does not answer the following questions: \u2022 How are rate limiting rules created? Where are the rules stored? \u2022 How to handle requests that are rate limited? In this section, we will first answer the questions regarding rate limiting rules and then go over the strategies to handle rate-limited requests. Finally, we will discuss rate limiting in distributed environment, a detailed design, performance optimization and monitoring. Rate limiting rules Lyft open-sourced their rate-limiting component [12]. We will peek inside of the component and look at some examples of rate limiting rules: domain : messaging descriptors : - key : message_type Value : marketing rate_limit : unit : day requests_per_unit : 5 In the above example, the system is configured to allow",
        ": marketing rate_limit : unit : day requests_per_unit : 5 In the above example, the system is configured to allow a maximum of 5 marketing messages per day. Here is another example: domain : auth descriptors : - key : auth_type Value : login rate_limit : unit : minute requests_per_unit : 5 This rule shows that clients are not allowed to login more than 5 times in 1 minute. Rules are generally written in configuration files and saved on disk. Exceeding the rate limit In case a request is rate limited, APIs return a HTTP response code 429 (too many requests) to the client. Depending on the use cases, we may enqueue the rate-limited requests to be processed later. For example, if some orders are rate limited due to system overload, we may keep those orders to be processed later. Rate limiter headers How does a client know whether it is being throttled? And how does a client know the number of allowed remaining requests before being throttled? The answer lies in HTTP response headers. The rate limiter returns the following HTTP headers to clients: X-Ratelimit-Remaining : The remaining number of allowed requests within the window. X-Ratelimit-Limit: It indicates how many calls the client can make per time window. X-Ratelimit-Retry-After: The number of seconds to wait until you can make a request again without being throttled. When a user has sent too many requests, a 429 too many requests error and X-Ratelimit- Retry-After header are returned to the client. Detailed design Figure 4-13 presents a detailed",
        "server-side API rate limiter? Interviewer : Great question. We focus on the server-side API rate limiter. Candidate : Does the rate limiter throttle API requests based on IP, the user ID, or other properties? Interviewer : The rate limiter should be flexible enough to support different sets of throttle rules. Candidate : What is the scale of the system? Is it built for a startup or a big company with a large user base? Interviewer : The system must be able to handle a large number of requests. Candidate : Will the system work in a distributed environment? Interviewer : Yes. Candidate : Is the rate limiter a separate service or should it be implemented in application code? Interviewer : It is a design decision up to you. Candidate : Do we need to inform users who are throttled? Interviewer : Yes. Requirements Here is a summary of the requirements for the system: \u2022 Accurately limit excessive requests. \u2022 Low latency. The rate limiter should not slow down HTTP response time. \u2022 Use as little memory as possible. \u2022 Distributed rate limiting. The rate limiter can be shared across multiple servers or processes. \u2022 Exception handling. Show clear exceptions to users when their requests are throttled. \u2022 High fault tolerance. If there are any problems with the rate limiter (for example, a cache server goes offline), it does not affect the entire system. Step 2 - Propose high-level design and get buy-in Let us keep things simple and use a basic client and server model for"
      ]
    },
    {
      "query": "What is Sliding window log algorithm?",
      "strategy": "multigranular",
      "response_time": 0.004998683929443359,
      "relevancy": 0.0271827021352141,
      "faithfulness": 0.45,
      "retrieved_chunks": [
        "it works. In Figure 4-8, the time unit is 1 second and the system allows a maximum of 3 requests per second. In each second window, if more than 3 requests are received, extra requests are dropped as shown in Figure 4-8. A major problem with this algorithm is that a burst of traffic at the edges of time windows could cause more requests than allowed quota to go through. Consider the following case: In Figure 4-9, the system allows a maximum of 5 requests per minute, and the available quota resets at the human-friendly round minute. As seen, there are five requests between 2:00:00 and 2:01:00 and five more requests between 2:01:00 and 2:02:00. For the one-minute window between 2:00:30 and 2:01:30, 10 requests go through. That is twice as many as allowed requests. Pros: \u2022 Memory efficient. \u2022 Easy to understand. \u2022 Resetting available quota at the end of a unit time window fits certain use cases. Cons: \u2022 Spike in traffic at the edges of a window could cause more requests than the allowed quota to go through. Sliding window log algorithm As discussed previously, the fixed window counter algorithm has a major issue: it allows more requests to go through at the edges of a window. The sliding window log algorithm fixes the issue. It works as follows: \u2022 The algorithm keeps track of request timestamps. Timestamp data is usually kept in cache, such as sorted sets of Redis [8]. \u2022 When a new request comes in, remove all the outdated timestamps.",
        "removed from the log. After the remove operation, the log size becomes 2; therefore, the request is accepted. Pros: \u2022 Rate limiting implemented by this algorithm is very accurate. In any rolling window, requests will not exceed the rate limit. Cons: \u2022 The algorithm consumes a lot of memory because even if a request is rejected, its timestamp might still be stored in memory. Sliding window counter algorithm The sliding window counter algorithm is a hybrid approach that combines the fixed window counter and sliding window log. The algorithm can be implemented by two different approaches. We will explain one implementation in this section and provide reference for the other implementation at the end of the section. Figure 4-11 illustrates how this algorithm works. Assume the rate limiter allows a maximum of 7 requests per minute, and there are 5 requests in the previous minute and 3 in the current minute. For a new request that arrives at a 30% position in the current minute, the number of requests in the rolling window is calculated using the following formula: \u2022 Requests in current window + requests in the previous window * overlap percentage of the rolling window and previous window \u2022 Using this formula, we get 3 + 5 * 0.7% = 6.5 request. Depending on the use case, the number can either be rounded up or down. In our example, it is rounded down to 6. Since the rate limiter allows a maximum of 7 requests per minute, the current request can go through. However,",
        "helps to choose the right algorithm or combination of algorithms to fit our use cases. Here is a list of popular algorithms: \u2022 Token bucket \u2022 Leaking bucket \u2022 Fixed window counter \u2022 Sliding window log \u2022 Sliding window counter Token bucket algorithm The token bucket algorithm is widely used for rate limiting. It is simple, well understood and commonly used by internet companies. Both Amazon [5] and Stripe [6] use this algorithm to throttle their API requests. The token bucket algorithm work as follows: \u2022 A token bucket is a container that has pre-defined capacity. Tokens are put in the bucket at preset rates periodically. Once the bucket is full, no more tokens are added. As shown in Figure 4-4, the token bucket capacity is 4. The refiller puts 2 tokens into the bucket every second. Once the bucket is full, extra tokens will overflow. \u2022 Each request consumes one token. When a request arrives, we check if there are enough tokens in the bucket. Figure 4-5 explains how it works. \u2022 If there are enough tokens, we take one token out for each request, and the request goes through. \u2022 If there are not enough tokens, the request is dropped. Figure 4-6 illustrates how token consumption, refill , and rate limiting logic work. In this example, the token bucket size is 4, and the refill rate is 4 per 1 minute. The token bucket algorithm takes two parameters: \u2022 Bucket size: the maximum number of tokens allowed in the bucket \u2022 Refill rate: number",
        "traffic like flash sales. In this scenario, we may replace the algorithm to support burst traffic. Token bucket is a good fit here. Step 4 - Wrap up In this chapter, we discussed different algorithms of rate limiting and their pros/cons. Algorithms discussed include: \u2022 Token bucket \u2022 Leaking bucket \u2022 Fixed window \u2022 Sliding window log \u2022 Sliding window counter Then, we discussed the system architecture, rate limiter in a distributed environment, performance optimization and monitoring. Similar to any system design interview questions, there are additional talking points you can mention if time allows: \u2022 Hard vs soft rate limiting. \u2022 Hard: The number of requests cannot exceed the threshold. \u2022 Soft: Requests can exceed the threshold for a short period. \u2022 Rate limiting at different levels. In this chapter, we only talked about rate limiting at the application level (HTTP: layer 7). It is possible to apply rate limiting at other layers. For example, you can apply rate limiting by IP addresses using Iptables [15] (IP: layer 3). Note: The Open Systems Interconnection model (OSI model) has 7 layers [16]: Layer 1: Physical layer, Layer 2: Data link layer, Layer 3: Network layer, Layer 4: Transport layer, Layer 5: Session layer, Layer 6: Presentation layer, Layer 7: Application layer. \u2022 Avoid being rate limited. Design your client with best practices: \u2022 Use client cache to avoid making frequent API calls. \u2022 Understand the limit and do not send too many requests in a short time frame. \u2022 Include code to catch exceptions or errors",
        "Write path Figure 6-19 explains what happens after a write request is directed to a specific node. Please note the proposed designs for write/read paths are primary based on the architecture of Cassandra [8]. 1. The write request is persisted on a commit log file. 2. Data is saved in the memory cache. 3. When the memory cache is full or reaches a predefined threshold, data is flushed to SSTable [9] on disk. Note: A sorted-string table (SSTable) is a sorted list of <key, value> pairs. For readers interested in learning more about SStable, refer to the reference material [9]. Read path After a read request is directed to a specific node, it first checks if data is in the memory cache. If so, the data is returned to the client as shown in Figure 6-20. If the data is not in memory, it will be retrieved from the disk instead. We need an efficient way to find out which SSTable contains the key. Bloom filter [10] is commonly used to solve this problem. The read path is shown in Figure 6-21 when data is not in memory. 1. The system first checks if data is in memory. If not, go to step 2. 2. If data is not in memory, the system checks the bloom filter. 3. The bloom filter is used to figure out which SSTables might contain the key. 4. SSTables return the result of the data set. 5. The result of the data set is returned to the client. Summary This chapter"
      ]
    }
  ]
}